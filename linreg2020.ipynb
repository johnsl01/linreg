{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linreg2020.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnsl01/linreg/blob/master/linreg2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UllJB8351q81"
      },
      "source": [
        " _Single linear regression - one dependent variable_\n",
        "\n",
        "y = B0 + B1.X\n",
        "\n",
        "X    np array of data points (with an addition col of leading Ones)\n",
        "\n",
        "y    np vector of results\n",
        "\n",
        "theta np vector of B0, B1  (or occasionally a tuple (B0,B1) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Fldw8f0uaRR"
      },
      "source": [
        "###############################################\n",
        "#@title                 IMPORTS               #\n",
        "###############################################\n",
        "print (\"Imports\")\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "from datetime import datetime\n",
        "print (datetime.now())\n",
        "\n",
        "from google.colab import files\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2rcNtbB00Of"
      },
      "source": [
        "==================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIZ5pRoMP7a1"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adqBJ-dHP9BX"
      },
      "source": [
        "print(\"def predict(X, theta)\")\n",
        "def predict(X, theta):\n",
        "  # takes m by n matrix X as input and returns an m by 1 vector \n",
        "  # containing the predictions h_theta(x^i) for each row x^i, i=1,...,m in X\n",
        "  ##### replace the next line with your code ##### \n",
        "\n",
        "  # Single variable example y = B0 + B1.x\n",
        "  #   The data point(s) include a leading 1 to assist with the \n",
        "  #   B0 coeficient.\n",
        "  \n",
        "  # Check for single data point\n",
        "  #   requires a special case as it arrives as 1D, rather than 2D\n",
        "  #   and if it does arrive as 2D the else handles it anyway.\n",
        "\n",
        "  # A single data point is trivial for regression\n",
        "  #   but the predict function should be generally usable for \n",
        "  #   any subset of the data including one data point.\n",
        "   \n",
        "  if len(X.shape) == 1 :\n",
        "    # this is edge case for single point arriving as a vector  \n",
        "    pred = X[0]*theta[0] + X[1]*theta[1]  \n",
        "\n",
        "  else : \n",
        "    #this the general case for data points in an array\n",
        "    pred = X[:,0]*theta[0] + X[:,1]*theta[1]  \n",
        "\n",
        "  # print(\"predict return : \" , pred)\n",
        "\n",
        "  return pred\n",
        "# end def predict\n",
        "\n",
        "\n",
        "print(\"def computeCost(X, y, theta)\")\n",
        "def computeCost(X, y, theta):\n",
        "  # function calculates the cost J(theta) and return its value\n",
        "  ##### replace the next line with your code #####\n",
        "  \n",
        "  # This function is already independent of the number of variables\n",
        "  #   provided the X array carries the variables with a leading 1 on data point(s)\n",
        "  #   and the theta has the same number of B0, B1, B2 ...  coeficients.\n",
        "  # However the predict it calls needs to be able to deal with the lenght of the theta \n",
        "  #   which corresponds to the number of variables + 1\n",
        "\n",
        "\n",
        "  # The sequential calculation could be done in fewer steps\n",
        "  # which would be more memory efficient but less clear.\n",
        "\n",
        "  # TODO  merge some calculations to tidy up the code and add efficiency\n",
        "  \n",
        "  # print (\"Compute Cost #1 : \" , X.shape, y.shape, theta.shape)\n",
        "\n",
        "  # Get the value predicted by the current theta values\n",
        "  costpred = predict (X,theta)\n",
        "  # print (\"Compute Cost #2 :\", costpred.shape)\n",
        "\n",
        "  # get the difference between the predictions and the actuals\n",
        "  costbase = costpred - y\n",
        "  # print (\"Compute Cost #3 :\", costbase.shape)\n",
        "\n",
        "  # get the square of the differences\n",
        "  costsq = costbase**2\n",
        "  # print (\"Compute Cost #4 :\", costsq.shape)\n",
        "\n",
        "  # get the sum of the squared differences\n",
        "  sumcost = costsq.sum()\n",
        "  # print (\"Compute Cost #5 :\", sumcost)\n",
        "\n",
        "  # print (y)\n",
        "  # print (costpred)\n",
        "  # print (\"Compute Cost #6 :\", len(y))  \n",
        "\n",
        "  # divide the sum of squares by twice the number of data points\n",
        "  cost = sumcost/(2*len(y))\n",
        "  # print (\"Compute Cost #7 :\", cost)  \n",
        "\n",
        "  # return the cost of the current theta values\n",
        "  return cost\n",
        "# end def computeCost\n",
        "\n",
        "\n",
        "\n",
        "print(\"def computeGradient(X, y, theta)\")\n",
        "def computeGradient(X, y, theta):\n",
        "  # function calulate the gradient of J(theta) and returns its value\n",
        "  ##### replace the next line with your code #####\n",
        "  \n",
        "  # Single variable simple version\n",
        "  \n",
        "  # initiate the result\n",
        "  grad = np.zeros(2)\n",
        "  \n",
        "  # print (\"Number of coeficients: \" , n)\n",
        "  # print (\"Shapes of data, result and theta : \" , X.shape, y.shape, theta.shape)\n",
        "  # print (\"Types of  data, result and theta : \" , type(X), type(y), type(theta))\n",
        "  # print (\"Sample head of data\")\n",
        "  # for i in range(5) :\n",
        "      # print (X[i,:])\n",
        "    \n",
        "  # Get the value predicted by the current theta values\n",
        "  costpred = predict (X,theta)\n",
        "  # print (\"Compute Grad #2 :\", costpred.shape)\n",
        "  \n",
        "  # get the difference between the predictions and the actuals\n",
        "  costbase = costpred - y\n",
        "  # print (\"Compute Grad #3 :\", costbase.shape)  \n",
        "  \n",
        "  # calculate the gradient for each coefficient\n",
        "  # for is inefficient but it is used to iterate over a small\n",
        "  # range (1 more than the number of variables \n",
        "  # i.e the number of coefficients)\n",
        "  # while numpy array maths are used for the larger dimension\n",
        "  # (the number of data points)\n",
        "  \n",
        "  # get product of differences by current coeficients data point\n",
        "  costprodb0 = costbase #  X[:,0] is all ones so don't multiply\n",
        "  costprodb1 = costbase * X[:,1]\n",
        "  # print (\"Compute Grad #4 :\", costprod.shape) \n",
        "  # get the sum of the cost products\n",
        "  costprodb0sum = costprodb0.sum()\n",
        "  costprodb1sum = costprodb1.sum()\n",
        "  # print (\"Compute Grad #5 :\", costprodsum)\n",
        "  # divide by the number of data points\n",
        "  grad[0] = costprodb0sum / len(y)\n",
        "  grad[1] = costprodb1sum / len(y)\n",
        "  # take this outside the for loop and use numpy maths ! \n",
        "  # print (\"Compute Grad #6 :\", grad[i])\n",
        "  # end for\n",
        "  # print (\"Compute Grad #6 :\", grad)\n",
        "  # return the gradient\n",
        "  return grad\n",
        "# end def computeGradient\n",
        "\n",
        "\n",
        "print (datetime.now())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vjjkR9X08NL"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPkP3zRP09AL"
      },
      "source": [
        "\n",
        "print(\"def gradientDescent(X, y, numparams)\")\n",
        "def gradientDescent(X, y, numparams):\n",
        "  # iteratively update parameter vector theta\n",
        "  # -- you should not modify this function\n",
        "\n",
        "  # initialize variables for learning rate and iterations\n",
        "  alpha = 0.02\n",
        "  iters = 10000\n",
        "  cost = np.zeros(iters)\n",
        "  theta= np.zeros(numparams)\n",
        "\n",
        "  for i in range(iters):\n",
        "    theta = theta - alpha * computeGradient(X,y,theta)\n",
        "    cost[i] = computeCost(X, y, theta)\n",
        "\n",
        "  return theta, cost\n",
        "\n",
        "\n",
        "\n",
        "print(\"def normaliseData(x)\")\n",
        "def normaliseData(x):\n",
        "  # rescale data to lie between 0 and 1\n",
        "  scale = x.max(axis=0)\n",
        "  return (x/scale, scale)\n",
        "\n",
        "\n",
        "\n",
        "print(\"def splitData(X, y)\")\n",
        "def splitData(X, y):\n",
        "  # split data into training and test parts\n",
        "  # ... for now, we use all of the data for training and testing\n",
        "  Xtrain=X; ytrain=y; Xtest=X; ytest=y\n",
        "  return (Xtrain, ytrain, Xtest, ytest)\n",
        "\n",
        "\n",
        "print (datetime.now())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqEOQP5Z0-J-"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH7808b81TMz"
      },
      "source": [
        "print (\"def main()\")\n",
        "def main():\n",
        "  # load the data\n",
        "  # \"https://raw.githubusercontent.com/johnsl01/Titanic_Python/master/titanic_known.csv\"\n",
        "  # data=np.loadtxt('https://raw.githubusercontent.com/johnsl01/linreg/master/stockprices.csv',usecols=(1,2))\n",
        "  # data=np.loadtxt('https://raw.githubusercontent.com/johnsl01/ML2020A/main/ML_week1.csv',usecols=(0,1))\n",
        "  data=np.loadtxt('https://raw.githubusercontent.com/johnsl01/ML2020A/main/ML_week1.csv', delimiter=',')\n",
        "  X=data[:,0]\n",
        "  y=data[:,1]\n",
        "\n",
        "  # plot the data so we can see how it looks\n",
        "  # (output is in file graph.png)\n",
        "  fig, ax = plt.subplots(figsize=(12, 8))\n",
        "  ax.scatter(X, y, label='Data')\n",
        "  ax.set_xlabel('X')\n",
        "  ax.set_ylabel('Y')\n",
        "  ax.set_title('X vs Y')\n",
        "  fig.savefig('graph.png')\n",
        "\n",
        "  # split the data into training and test parts\n",
        "  (Xtrain, ytrain, Xtest, ytest)=splitData(X,y)\n",
        "\n",
        "  # add a column of ones to input data\n",
        "  m=len(y) # m is number of training data points\n",
        "  Xtrain = np.column_stack((np.ones((m, 1)), Xtrain))\n",
        "  (m,n)=Xtrain.shape # m is number of data points, n number of features\n",
        "\n",
        "  # rescale training data to lie between 0 and 1\n",
        "  (Xt,Xscale) = normaliseData(Xtrain)\n",
        "  (yt,yscale) = normaliseData(ytrain)\n",
        "\n",
        "  # calculate the prediction\n",
        "  '''\n",
        "  print('testing the prediction function ...')\n",
        "  theta=(1,2)\n",
        "  print('when x=[1,1] and theta is [1,2]) cost = ',predict(np.ones(n),theta))\n",
        "  print('approx expected prediction is 3')\n",
        "  print('when x=[[1,1],[5,5]] and theta is [1,2]) cost = ',predict(np.array([[1,1],[5,5]]),theta))\n",
        "  print('approx expected prediction is [3,15]')\n",
        "  # input('Press Enter to continue...')\n",
        "  '''\n",
        "\n",
        "  '''\n",
        "  # calculate the cost when theta iz zero\n",
        "  print('testing the cost function ...')\n",
        "  theta=np.zeros(n)\n",
        "  print('when theta is zero cost = ',computeCost(Xt,yt,theta))\n",
        "  print('approx expected cost value is 0.318')\n",
        "  # input('Press Enter to continue...')\n",
        "  '''\n",
        "\n",
        "  '''\n",
        "  # calculate the gradient when theta is zero\n",
        "  print('testing the gradient function ...')\n",
        "  print('when theta is zero gradient = ',computeGradient(Xt,yt,theta))\n",
        "  print('approx expected gradient value is [-0.79,-0.59]')\n",
        "  # input('Press Enter to continue...')\n",
        "  '''\n",
        "\n",
        "\n",
        "  # perform gradient descent to \"fit\" the model parameters\n",
        "  print('running gradient descent ...')\n",
        "  theta, cost = gradientDescent(Xt, yt, n)\n",
        "  print('after running gradientDescent() theta=',theta)\n",
        "  print('approx expected value is [-1.1703  1.6582]')\n",
        "\n",
        "  # plot some predictions\n",
        "  Xpred = np.linspace(X.min(), X.max(), 100)\n",
        "  Xpred = np.column_stack((np.ones((100, 1)), Xpred))\n",
        "  ypred = predict(Xpred/Xscale, theta)*yscale\n",
        "  fig, ax = plt.subplots(figsize=(12, 8))\n",
        "  ax.scatter(Xtest, ytest, color='b', label='Test Data')\n",
        "  ax.plot(Xpred[:,1], ypred, 'r', label='Prediction')\n",
        "  ax.set_xlabel('X')\n",
        "  ax.set_ylabel('Y')\n",
        "  ax.legend(loc=2)\n",
        "  fig.savefig('pred.png')\n",
        "\n",
        "  # and plot how the cost varies as the gradient descent proceeds\n",
        "  fig2, ax2 = plt.subplots(figsize=(12, 8))\n",
        "  ax2.semilogy(cost,'r')\n",
        "  ax2.set_xlabel('iteration')\n",
        "  ax2.set_ylabel('cost')\n",
        "  fig2.savefig('cost.png')\n",
        "  \n",
        "  # plot the cost function\n",
        "  fig3 = plt.figure()\n",
        "  ax3 = fig3.add_subplot(1, 1, 1, projection='3d')\n",
        "  n=100\n",
        "  theta0, theta1 = np.meshgrid(np.linspace(-3, 3, n), np.linspace(-3, 2, n))\n",
        "  cost = np.empty((n,n))\n",
        "  for i in range(n):\n",
        "    for j in range(n):\n",
        "      cost[i,j] = computeCost(Xt,yt,(theta0[i,j],theta1[i,j]))\n",
        "  ax3.plot_surface(theta0,theta1,cost)\n",
        "  ax3.set_xlabel('theta0')\n",
        "  ax3.set_ylabel('theta1')\n",
        "  ax3.set_zlabel('J(theta)')\n",
        "  fig3.savefig('J.png')\n",
        "  \n",
        "  '''\n",
        "  inpchar = input('Press Y and Enter to download images of graphs')\n",
        "  \n",
        "  if inpchar == \"Y\" :\n",
        "    files.download('graph.png')\n",
        "    files.download('pred.png')\n",
        "    files.download('cost.png')\n",
        "    files.download('J.png')\n",
        "  else :\n",
        "    print (\"Bypassing graph image download\")\n",
        "  '''\n",
        "\n",
        "# end def main()  \n",
        "  \n",
        "print (datetime.now())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNsoCBRt1bha"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiLpSKwE1ckc"
      },
      "source": [
        "print (datetime.now())\n",
        "\n",
        "main()\n",
        "\n",
        "print (datetime.now())\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}

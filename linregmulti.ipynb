{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linreg.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnsl01/linreg/blob/master/linregmulti.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Fldw8f0uaRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###############################################\n",
        "#@title                 IMPORTS               #\n",
        "###############################################\n",
        "print (\"Imports\")\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "from datetime import datetime\n",
        "print (datetime.now())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2rcNtbB00Of",
        "colab_type": "text"
      },
      "source": [
        "==================================================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHlJ3giB07Un",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# where X is a 2D np.array and theta is tuple\n",
        "# returns h_theta_x a 1D np.array\n",
        "print (\"def h_theta(X,theta)\")\n",
        "def h_theta(X,theta):\n",
        "  (_,n) = X.shape\n",
        "  t = len(theta)\n",
        "  if (t is n+1):\n",
        "    theta_np = np.array(theta[1:(t)])\n",
        "    theta0 = theta[0]\n",
        "  elif (t is n):\n",
        "    theta_np = np.array(theta[0:(t)])\n",
        "    theta0 = 0\n",
        "  print(X)\n",
        "  print(theta_np)\n",
        "\n",
        "    # h_theta = theta0 + theta1*X[1] + ...\n",
        "  h_theta_x = np.multiply(theta_np, X)\n",
        "  print(h_theta_x)\n",
        "    # for each row in X, h_theta[i] = X[i] * thetaN\n",
        "  h_theta_x = np.add(h_theta_x, theta0)\n",
        "  print(h_theta_x)\n",
        "    # for each element in h_theta, h_theta[i] += theta0\n",
        "  return h_theta_x\n",
        "#end h_theta\n",
        "\n",
        "# where X is a 2D np.array, and y is a 1D array and theta is a tuple\n",
        "print(\"def linearRegression(X, y, theta)\")\n",
        "def linearRegression(X, y, theta):\n",
        "  (m,_) = X.shape # extracts dimensions\n",
        "  h_theta_x = h_theta(X, theta)\n",
        "  diff = np.subtract(h_theta_x, y)\n",
        "    # for each element, h_theta[i] - y[i]\n",
        "  total_error = np.sum(np.multiply(diff, diff))\n",
        "    # squares each element in diff and sums them to find total error\n",
        "  j_theta = total_error/(2*m)\n",
        "  return j_theta\n",
        "# end linearRegression\n",
        "\n",
        "\n",
        "# assumes that X is a 2D np.array, and y and theta are 1D np.arrays\n",
        "print(\"def gradientDescent_1(X, y, theta_in, alpha)\")\n",
        "def gradientDescent_1(X, y, theta_in, alpha):\n",
        "  (m, _) = X.shape\n",
        "  h_theta_x = h_theta(X, theta_in)\n",
        "  diff = np.subtract(h_theta_x, y)\n",
        "    # for each element, h_theta[i] - y[i]\n",
        "  prodsum = np.sum(np.multiply(diff,X))\n",
        "\n",
        "  theta_out = theta_in - (alpha*prodsum)/m\n",
        "  return theta_out\n",
        "# end gradientDescent_1\n",
        "\n",
        "\n",
        "print (datetime.now())\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIZ5pRoMP7a1",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adqBJ-dHP9BX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"def predict(X, theta)\")\n",
        "def predict(X, theta):\n",
        "  # takes m by n matrix X as input and returns an m by 1 vector \n",
        "  # containing the predictions h_theta(x^i) for each row x^i, i=1,...,m in X\n",
        "  ##### replace the next line with your code #####\n",
        "  # print(X)\n",
        "  # print(theta)\n",
        "  # print(X.shape)\n",
        "  n=len(theta)\n",
        "  \n",
        "  if n>2 :\n",
        "    pred = predictmulti(X, theta)\n",
        "  else :  \n",
        "  \n",
        "  # for the multiple linear regression we need to iterate over the \n",
        "  # length of theta \n",
        "  # this assumes a simple B0 + B1.X1 + B2.X2 + ....\n",
        "  # it does not consider more complex dependencies\n",
        "  # such as B0 + B1.X1 + B2.X2 + B3.X1.X2   etc.\n",
        "  # these require the X to be constructed to meet the requirement\n",
        "  # and introduce too much colinearity.\n",
        "  \n",
        "  # check for single data point\n",
        "  # requires a special case as it arrives as 1D, rather than 2D\n",
        "  # and if it does arrive as 2D the else handles it anyway\n",
        "   \n",
        "  # this the general case for multiple data points in 2D\n",
        "    if len(X.shape) == 1 : \n",
        "      pred = X[0]*theta[0] + X[1]*theta[1]   \n",
        "    else : \n",
        "      #this the general case for multiple data points in 2D\n",
        "      pred = X[:,0]*theta[0] + X[:,1]*theta[1]      \n",
        "  # print(pred)\n",
        "  return pred\n",
        "# end def predict\n",
        "\n",
        "\n",
        "print(\"def predictmulti(X, theta)\")\n",
        "def predictmulti(X, theta):\n",
        "  # takes m by n matrix X as input and returns an m by 1 vector \n",
        "  # containing the predictions h_theta(x^i) for each row x^i, i=1,...,m in X\n",
        "  ##### replace the next line with your code #####\n",
        "  # print(X)\n",
        "  # print(theta)\n",
        "  # print(X.shape)\n",
        "  \n",
        "  # number of coeficients\n",
        "  n=len(theta)\n",
        "  \n",
        "  # for the multiple linear regression we need to iterate over the \n",
        "  # length of theta \n",
        "  # this assumes a simple B0 + B1.X1 + B2.X2 + ....\n",
        "  # it does not consider more complex dependencies\n",
        "  # such as B0 + B1.X1 + B2.X2 + B3.X1.X2   etc.\n",
        "  # these require the X to be constructed to meet the requirement\n",
        "  # and introduce too much colinearity.\n",
        "  \n",
        "  # check for single data point\n",
        "  # requires a special case as it arrives as 1D, rather than 2D\n",
        "  # and if it does arrive as 2D the else handles it anyway\n",
        "  \n",
        " \n",
        "  \n",
        "  # this the general case for multiple data points in 2D\n",
        "  if len(X.shape) == 1 :\n",
        "    pred = X[0]*theta[0]\n",
        "    for i in range(1,n) :\n",
        "      pred = pred + X[i]*theta[i]\n",
        "  else :  \n",
        "    pred = X[:,0]*theta[0]  \n",
        "    for i in range(1,n) :\n",
        "      pred = pred + X[:,i]*theta[i] \n",
        "    \n",
        "  # print(pred)\n",
        "  return pred\n",
        "# end def predict\n",
        "\n",
        "\n",
        "print(\"def computeCost(X, y, theta)\")\n",
        "def computeCost(X, y, theta):\n",
        "  # function calculates the cost J(theta) and return its value\n",
        "  ##### replace the next line with your code #####\n",
        "  \n",
        "  # this function is already independent of the number of variables\n",
        "  # provides the X array carries the variables with a leading 1 on data point\n",
        "  # and the theta has the same number of B0, B1, B2 ...  values.\n",
        "  \n",
        "  # print (\"Compute Cost #1 : \" , X.shape, y.shape, theta.shape)\n",
        "  # Get the value predicted by the current theta values\n",
        "  costpred = predict (X,theta)\n",
        "  # print (\"Compute Cost #2 :\", costpred.shape)\n",
        "  # get the difference between the predictions and the actuals\n",
        "  costbase = costpred - y\n",
        "  # print (\"Compute Cost #3 :\", costbase.shape)\n",
        "  # get the square of the differences\n",
        "  costsq = costbase **2\n",
        "  # print (\"Compute Cost #4 :\", costsq.shape)\n",
        "  # get the sum of the squared differences\n",
        "  sumcost = costsq.sum()\n",
        "  # print (\"Compute Cost #5 :\", sumcost)\n",
        "  # print (y)\n",
        "  # print (costpred)\n",
        "  # print (\"Compute Cost #6 :\", len(y))  \n",
        "  # divide the sum of squares by twice the number of data points\n",
        "  cost = sumcost/(2*len(y))\n",
        "  # print (\"Compute Cost #7 :\", cost)    \n",
        "  # cost = 0\n",
        "  # return the cost of the current theta values\n",
        "  return cost\n",
        "# end def computeCost\n",
        "\n",
        "\n",
        "\n",
        "print(\"def computeGradient(X, y, theta)\")\n",
        "def computeGradient(X, y, theta):\n",
        "  # function calulate the gradient of J(theta) and returns its value\n",
        "  ##### replace the next line with your code #####\n",
        "  # This function is already capable of delaing with multiple varables\n",
        "  # provided X (with a leading 1 per data point) and theta are matched sizes.\n",
        "  \n",
        "  # do we need a simpler version for the single variable case.\n",
        "  \n",
        "  # number of coeficients\n",
        "  n=len(theta)\n",
        "  # initiate the result\n",
        "  grad = np.zeros(n)\n",
        "  \n",
        "  # print (\"Number of coeficients: \" , n)\n",
        "  # print (\"Shapes of data, result and theta : \" , X.shape, y.shape, theta.shape)\n",
        "  # print (\"Types of  data, result and theta : \" , type(X), type(y), type(theta))\n",
        "  # print (\"Sample head of data\")\n",
        "  # for i in range(5) :\n",
        "      # print (X[i,:])\n",
        "    \n",
        "  # Get the value predicted by the current theta values\n",
        "  costpred = predict (X,theta)\n",
        "  # print (\"Compute Grad #2 :\", costpred.shape)\n",
        "  \n",
        "  # get the difference between the predictions and the actuals\n",
        "  costbase = costpred - y\n",
        "  # print (\"Compute Grad #3 :\", costbase.shape)  \n",
        "  \n",
        "  # calculate the gradient for each coefficient\n",
        "  # for is inefficient but it is used to iterate over a small\n",
        "  # range (1 more than the number of variables \n",
        "  # i.e the number of coefficients)\n",
        "  # while numpy array maths are used for the larger dimension\n",
        "  # (the number of data points)\n",
        "  for i in range(n) :\n",
        "    # get product of differences by current coeficients data point\n",
        "    costprod = costbase * X[:,i]\n",
        "    # print (\"Compute Grad #4 :\", costprod.shape) \n",
        "    # get the sum of the cost products\n",
        "    costprodsum = costprod.sum()\n",
        "    # print (\"Compute Grad #5 :\", costprodsum)\n",
        "    # divide by the number of data points\n",
        "    grad[i] = costprodsum / len(y)\n",
        "    # take this outside the for loop and use numpy maths ! \n",
        "    # print (\"Compute Grad #6 :\", grad[i])\n",
        "  # end for\n",
        "  # print (\"Compute Grad #6 :\", grad)\n",
        "  # return the gradient\n",
        "  return grad\n",
        "# end def computeGradient\n",
        "\n",
        "\n",
        "print (datetime.now())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vjjkR9X08NL",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPkP3zRP09AL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(\"def gradientDescent(X, y, numparams)\")\n",
        "def gradientDescent(X, y, numparams):\n",
        "  # iteratively update parameter vector theta\n",
        "  # -- you should not modify this function\n",
        "\n",
        "  # initialize variables for learning rate and iterations\n",
        "  alpha = 0.02\n",
        "  iters = 5000\n",
        "  cost = np.zeros(iters)\n",
        "  theta= np.zeros(numparams)\n",
        "\n",
        "  for i in range(iters):\n",
        "    theta = theta - alpha * computeGradient(X,y,theta)\n",
        "    cost[i] = computeCost(X, y, theta)\n",
        "\n",
        "  return theta, cost\n",
        "\n",
        "\n",
        "\n",
        "print(\"def normaliseData(x)\")\n",
        "def normaliseData(x):\n",
        "  # rescale data to lie between 0 and 1\n",
        "  scale = x.max(axis=0)\n",
        "  return (x/scale, scale)\n",
        "\n",
        "\n",
        "\n",
        "print(\"def splitData(X, y)\")\n",
        "def splitData(X, y):\n",
        "  # split data into training and test parts\n",
        "  # ... for now, we use all of the data for training and testing\n",
        "  Xtrain=X; ytrain=y; Xtest=X; ytest=y\n",
        "  return (Xtrain, ytrain, Xtest, ytest)\n",
        "\n",
        "\n",
        "print (datetime.now())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqEOQP5Z0-J-",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH7808b81TMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (\"def main()\")\n",
        "def main():\n",
        "  # load the data\n",
        "  # \"https://raw.githubusercontent.com/johnsl01/Titanic_Python/master/titanic_known.csv\"\n",
        "  data=np.loadtxt('https://raw.githubusercontent.com/johnsl01/linreg/master/stockprices.csv',usecols=(1,2))\n",
        "  X=data[:,0]\n",
        "  y=data[:,1]\n",
        "\n",
        "  # plot the data so we can see how it looks\n",
        "  # (output is in file graph.png)\n",
        "  fig, ax = plt.subplots(figsize=(12, 8))\n",
        "  ax.scatter(X, y, label='Data')\n",
        "  ax.set_xlabel('Amazon')\n",
        "  ax.set_ylabel('Google')\n",
        "  ax.set_title('Google stock price vs Amazon')\n",
        "  fig.savefig('graph.png')\n",
        "\n",
        "  # split the data into training and test parts\n",
        "  (Xtrain, ytrain, Xtest, ytest)=splitData(X,y)\n",
        "\n",
        "  # add a column of ones to input data\n",
        "  m=len(y) # m is number of training data points\n",
        "  Xtrain = np.column_stack((np.ones((m, 1)), Xtrain))\n",
        "  (m,n)=Xtrain.shape # m is number of data points, n number of features\n",
        "\n",
        "  # rescale training data to lie between 0 and 1\n",
        "  (Xt,Xscale) = normaliseData(Xtrain)\n",
        "  (yt,yscale) = normaliseData(ytrain)\n",
        "\n",
        "  # calculate the prediction\n",
        "  print('testing the prediction function ...')\n",
        "  theta=(1,2)\n",
        "  print('when x=[1,1] and theta is [1,2]) cost = ',predict(np.ones(n),theta))\n",
        "  print('approx expected prediction is 3')\n",
        "  print('when x=[[1,1],[5,5]] and theta is [1,2]) cost = ',predict(np.array([[1,1],[5,5]]),theta))\n",
        "  print('approx expected prediction is [3,15]')\n",
        "  input('Press Enter to continue...')\n",
        "\n",
        "  # calculate the cost when theta iz zero\n",
        "  print('testing the cost function ...')\n",
        "  theta=np.zeros(n)\n",
        "  print('when theta is zero cost = ',computeCost(Xt,yt,theta))\n",
        "  print('approx expected cost value is 0.318')\n",
        "  input('Press Enter to continue...')\n",
        "\n",
        "  # calculate the gradient when theta is zero\n",
        "  print('testing the gradient function ...')\n",
        "  print('when theta is zero gradient = ',computeGradient(Xt,yt,theta))\n",
        "  print('approx expected gradient value is [-0.79,-0.59]')\n",
        "  input('Press Enter to continue...')\n",
        "\n",
        "  # perform gradient descent to \"fit\" the model parameters\n",
        "  print('running gradient descent ...')\n",
        "  theta, cost = gradientDescent(Xt, yt, n)\n",
        "  print('after running gradientDescent() theta=',theta)\n",
        "  print('approx expected value is [0.34, 0.61]')\n",
        "\n",
        "  # plot some predictions\n",
        "  Xpred = np.linspace(X.min(), X.max(), 100)\n",
        "  Xpred = np.column_stack((np.ones((100, 1)), Xpred))\n",
        "  ypred = predict(Xpred/Xscale, theta)*yscale\n",
        "  fig, ax = plt.subplots(figsize=(12, 8))\n",
        "  ax.scatter(Xtest, ytest, color='b', label='Test Data')\n",
        "  ax.plot(Xpred[:,1], ypred, 'r', label='Prediction')\n",
        "  ax.set_xlabel('Amazon')\n",
        "  ax.set_ylabel('Google')\n",
        "  ax.legend(loc=2)\n",
        "  fig.savefig('pred.png')\n",
        "\n",
        "  # and plot how the cost varies as the gradient descent proceeds\n",
        "  fig2, ax2 = plt.subplots(figsize=(12, 8))\n",
        "  ax2.semilogy(cost,'r')\n",
        "  ax2.set_xlabel('iteration')\n",
        "  ax2.set_ylabel('cost')\n",
        "  fig2.savefig('cost.png')\n",
        "  \n",
        "  # plot the cost function\n",
        "  fig3 = plt.figure()\n",
        "  ax3 = fig3.add_subplot(1, 1, 1, projection='3d')\n",
        "  n=100\n",
        "  theta0, theta1 = np.meshgrid(np.linspace(-3, 3, n), np.linspace(-3, 2, n))\n",
        "  cost = np.empty((n,n))\n",
        "  for i in range(n):\n",
        "    for j in range(n):\n",
        "      cost[i,j] = computeCost(Xt,yt,(theta0[i,j],theta1[i,j]))\n",
        "  ax3.plot_surface(theta0,theta1,cost)\n",
        "  ax3.set_xlabel('theta0')\n",
        "  ax3.set_ylabel('theta1')\n",
        "  ax3.set_zlabel('J(theta)')\n",
        "  fig3.savefig('J.png')\n",
        "  \n",
        "# end def main()  \n",
        "  \n",
        "print (datetime.now())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNsoCBRt1bha",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiLpSKwE1ckc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (datetime.now())\n",
        "\n",
        "main()\n",
        "\n",
        "print (datetime.now())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpP7InSLDUl4",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjI-DG_EDV76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test section \n",
        "# assume all defs are in place \n",
        "# but main hasn't run\n",
        "\n",
        "test_array = np.array([[1,3],[5,7],[11,13]])\n",
        "print ( test_array.shape )\n",
        "test_theta = np.array([1,2])\n",
        "print ( test_theta.shape )\n",
        "\n",
        "test_predict = predict (test_array, test_theta)\n",
        "print(test_predict)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNFAS80nGUHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (test_array)\n",
        "print (test_array * 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLVjtO73Gekf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (test_array + test_array)\n",
        "print (test_array[0,:])\n",
        "print (test_array[1,:])\n",
        "print (test_array[2,:])\n",
        "print (test_array[:,0])\n",
        "print (test_array[:,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOFsB7PkWMgo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_1 = np.zeros((test_array.shape[0],test_array.shape[1]))\n",
        "print (predict_1.shape)\n",
        "print (predict_1)\n",
        "\n",
        "print (predict_1[0,:])\n",
        "print (predict_1[1,:])\n",
        "print (predict_1[2,:])\n",
        "print (predict_1[:,0])\n",
        "print (predict_1[:,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7KElaA90mQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testnum = \"Case #1\"\n",
        "myX = np.array([1,1])\n",
        "mytheta = (1,2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_2lfFOB9p-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testnum = \"Case #2\"\n",
        "myX = np.array([[1,1],[5,5]])\n",
        "mytheta = (1,2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NEBjvfq-Z4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (testnum)\n",
        "print (datetime.now())\n",
        "print (\"Shape of myX : \", myX.shape)\n",
        "print (\"Type of myX.shape : \" , type (myX.shape))\n",
        "print (\"Len of myX.shape : \", len(myX.shape))\n",
        "if len(myX.shape) == 1 : \n",
        "  print (\"myX col 0 : \", myX[0])\n",
        "  print (\"myX col 1 : \", myX[1])\n",
        "else : \n",
        "  print (\"myX col 0 : \", myX[:,0])\n",
        "  print (\"myX col 1 : \", myX[:,1])\n",
        "print (mytheta[0])\n",
        "print (mytheta[1])\n",
        "mypredict_1 = predict(myX,mytheta)\n",
        "print (mypredict_1.shape)\n",
        "print (mypredict_1)\n",
        "print (datetime.now())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoHThVczECeX",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ie7Prpo2EDFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# multi dimensional test example\n",
        "# need two dependant variables \n",
        "# will produce a planar regression.\n",
        "# need some data\n",
        "# use the existing amazon and google data \n",
        "# but generate a new result variable\n",
        "# based on an integer multiple of each of the variables \n",
        "# with a randon factor thrown into each plus an additional \n",
        "# randon factor.\n",
        "data=np.loadtxt('https://raw.githubusercontent.com/johnsl01/linreg/master/stockprices.csv',usecols=(1,2))\n",
        "# X=data[:,0]\n",
        "# y=data[:,1]\n",
        "X = data\n",
        "y = X[:,0]*3 \n",
        "y = y + X[:,1]*2\n",
        "print (X.shape, y.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}